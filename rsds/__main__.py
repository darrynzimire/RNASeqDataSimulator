# encoding=utf-8


import os, warnings
import sys
import numpy as np
import gzip
import random
import itertools
from Bio.Seq import Seq
import pyfaidx
from rsds import SequenceContainer
from rsds import process_inputFiles
from rsds import distributions, cigar
import argparse
import logging.handlers
from datetime import datetime
from rsds import man
import time
from rsds import Validator
from pathlib import Path


if not sys.warnoptions:

	warnings.simplefilter("default")  # Change the filter in this process
	os.environ["PYTHONWARNINGS"] = "default"  # Also affect subprocesses
	warnings.simplefilter("ignore", ResourceWarning)

# Make a global logging object.
errlog = logging.getLogger("ErrLog")

# Set logging level, and write everything to a file
errlog.setLevel(logging.DEBUG)
LOG_FILENAME = './err.log'
h = logging.FileHandler(LOG_FILENAME, 'w')
f = logging.Formatter("%(levelname)s %(asctime)s %(funcName)s %(lineno)d %(message)s")
h.setFormatter(f)
errlog.addHandler(h)


def get_arguments():

	parser = argparse.ArgumentParser()

	parser.add_argument('-r', type=int, required=False, default=101)
	parser.add_argument('-n', type=int, required=False)
	parser.add_argument('-f', type=str, required=False)
	parser.add_argument('-s', type=int, required=False, default=1223)
	parser.add_argument('-o', type=str, required=False)
	parser.add_argument('-q', type=str, required=False)
	parser.add_argument('-c', type=str, required=False)
	parser.add_argument('-er', type=float, required=False, default=-1)
	parser.add_argument('-mm', nargs=2, type=int, required=False, default=(250, 25))
	parser.add_argument('-fl', type=str, required=False)

	parser.add_argument('-se', action='store_true', required=False)
	parser.add_argument('-pe', action='store_true', required=False)

	return parser


argparser = get_arguments()
args = argparser.parse_args()

(fragment_size, fragment_std) = args.mm
fl_model = args.fl
ref = args.f
readlen = args.r
readtot = args.n
seed = args.s
output = args.o
sqmodel = args.q
countModel = args.c
SE_RATE = args.er


def parseIndexRef(indexFile):
	"""
	Description:
	Read in sequence data from reference index FASTA file returns a list of transcript IDs
	offset, seqLen, position
	Parameters
	 - indexFile (str): The index file generated by the program, written to the current directory
	Return: The function returns a list of tuples containing the transcript id, start and end offset of the transcript sequence
	"""
	ref_inds = []
	filt_ref_inds = []

	try:

		fai = open(indexFile, 'r')
	except BaseException:
		print()
		errlog.error('Cannot find indexed reference file. Please provide a reference FASTA file')
		sys.exit('Cannot find indexed reference file. Please provide a reference FASTA file')

	for line in fai:
		splt = line[:-1].split('\t')
		header = '@' + splt[0]
		seqLen = int(splt[1])
		offset = int(splt[2])
		lineLn = int(splt[3])
		nLines = seqLen / lineLn

		if seqLen % lineLn != 0:
			nLines += 1
		ref_inds.append([header, offset, offset + seqLen + nLines, seqLen])
	for i in ref_inds:
		if i[3] >= 400:
			filt_ref_inds.append(i)
	for x in filt_ref_inds:
		x.pop(3)
	fai.close()
	return filt_ref_inds


def samplingtranscripts(ids):
	""""
	Description: This function randomly sample from all reference transcripts
	Parameters: ids (list of tuples) It takes as input all reference transcripts offsets
	Returns: This function returns a subset of transcript ids to be sampled from
	"""
	random.seed(seed)
	numreads = readtot
	sampledtranscripts = random.sample(ids, numreads)

	return sampledtranscripts


def scalereadnum(read_counts, n):
	sc = []
	scale = []
	total = sum(read_counts)
	for i in read_counts:
		x = i / total
		scale.append(x)
	for i in scale:
		y = n * i
		sc.append(round(y))
	scaled_counts = [1 if x == 0 else x for x in sc]

	return scaled_counts


def getseq(key, start=1, end=None):
	"""
	Description
	Get a sequence by key coordinates are 1-based and end is inclusive
	Parameters:
		key:
		start:
		end:
	Returns:
	"""

	if end != None and end < start:
		return ""
	start -= 1
	seek = start

	# if seek is past sequence then return empty sequence
	if seek >= end:
		return ""

	# seek to beginning
	infile = open(ref, 'r')
	infile.seek(seek)

	# read until end of sequence
	header = ''
	seq = []
	if end == None:
		lenNeeded = util.INF
	else:
		lenNeeded = end - start

	len2 = 0
	while len2 < lenNeeded:
		line = infile.readline()
		if line.startswith(">") or len(line) == 0:
			break
		seq.append(header + line.rstrip())
		len2 += len(seq[-1])
		if len2 > lenNeeded:
			seq[-1] = seq[-1][:-int(len2 - lenNeeded)]
			break
	seq = "".join(seq)
	return seq


def processTransIDs(ids):

	""""
	Description:
	This function take as input a list of transcript ids and converts it to a dictionary
	Parameters:
		ids (list of tuples): List of transcript ids
	Returns: The function returns a dictionary of transcript id as key and start and end position as value
	"""

	Transseq = []
	header = []
	transcriptID = {i: [j, k] for i, j, k in ids}
	ID = transcriptID.keys()
	for k in ID:
		header.append(k)
	pos = transcriptID.values()
	for i in pos:
		start = i[0]
		end = i[1]
		seq = getseq(ID, start, end)
		Transseq.append(seq)

	new_dict = {k: v for k, v in zip(header, Transseq)}
	return new_dict


def GenerateRead(seq, readLen, n, *args):
	"""
	Description:
	This function truncates transcript sequences by a specified read length.
	Parameters:
	:param seq: Transcript sequence randomly sampled from the input reference transcriptome file
	:param readLen: The user-specified read length

	:return: The function returns a list of all truncated sequences
	"""

	seqLen = len(seq)

	spos = []
	epos = []
	for ag in args:

		if ag == 'SE':

			nmax = seqLen - readLen - 1
			v = np.round(np.random.uniform(low=0, high=nmax, size=n))
			startpos = list(random.choices(v, k=n))
			endpos = [i + readLen for i in startpos]
			spos.append(startpos)
			epos.append(endpos)

		elif ag == 'PE':

			nmax = [seqLen - i - 1 for i in readLen]
			v = np.round(np.random.uniform(low=0, high=nmax, size=None))
			startpos = list(random.choices(v, k=len(readLen)))
			endpos = [i + j for i, j in zip(startpos, readLen)]
			spos.append(startpos)
			epos.append(endpos)

	return spos, epos


def reverse_complement(inputread):
	s = Seq(str(inputread))
	read = s.reverse_complement()
	return read


SE_CLASS = SequenceContainer.ReadContainer(readlen, sqmodel, SE_RATE)


def sample_qualscore(sequencingModel):
	(myQual, myErrors) = SE_CLASS.getSequencingErrors(sequencingModel)

	return myQual


def sequence_identifier(index):
	header = '{}{} {} {}{}'.format('@RSDS_v0.1.', index, index, 'length=', str(readlen))
	return header


def get_reads(record):
	start = record[0]
	end = record[1]
	sequence = record[2]
	reads = []
	for s, e in zip(start, end):
		r = sequence[int(s):int(e)]
		reads.append(r)

	return reads


def process_reads_PE(fragment, index):

	R1 = []
	R2 = []
	prob = str(np.random.rand(1)).lstrip('[').rstrip(']')
	read1 = ''.join(map(str, fragment[:readlen]))
	read2 = str(reverse_complement(fragment[-readlen:]))
	if float(prob) < 0.5:
		R1.append(read2)
		R2.append(read1)
	else:
		R1.append(read1)
		R2.append(read2)

	return R1, R2


start_time = datetime.now()


def main():

	if ref == None:
		man.manpage()
		sys.exit()

	else:

		errlog.info(print('reading reference file: ' + str(ref) + "\n"))
		errlog.info(print('Indexing reference file....' + "\n"))

		# First check if the working directory have permission to create a symlink
		# Check if working directory has reading permission
		# cwd = os.getcwd()
		# Use basename of the input reference FASTA as prefix for the symlink
		# check if windows is the OS, if TRUE print(info) output
		# check if the reference file in wd, if FALSE, print(info) exit
		
		# try:
		# 	Validator
		#
		# except OSError: print('You do not have permission to write or read in this directory. \n'
		# 					  'Please copy the reference FASTA file into your current working directory and re-run')
		#
		# folder_time = datetime.now().strftime('%Y-%m-%d_%I:%M:%S')
		basename = str(os.path.basename(output))
		os.symlink(ref, basename)
		pyfaidx.Faidx(basename)
		cwd = os.getcwd()
		indexFile = ''
		for file in os.listdir(cwd):
			if file.endswith('.fai'):
				indexFile = (os.path.join('.', file))
		
	ref_transcript_ids = parseIndexRef(indexFile)

	if args.se:

		sample_trans_ids = []
		COUNTS = []
		ID = []
		Seq = []

		if countModel == None:
			errlog.info(print('Simulating single-end reads....' + "\n"))
			errlog.info(print('No transcript profile model detected!!' + "\n"))
			errlog.info(print('Simulating default transcript profile' + "\n"))

			NB_counts = distributions.negative_binomial()
			counts_NB = np.random.choice(NB_counts, size=readtot, replace=True).tolist()
			scaled_counts = scalereadnum(counts_NB, readtot)
			samptransids = random.choices(ref_transcript_ids, k=len(scaled_counts))
			sample_trans_ids.append(samptransids)
			COUNTS.append(scaled_counts)

		elif countModel != None and readtot == None:
			errlog.info(print('Simulating single-end reads....' + "\n"))
			errlog.info(print('Detected transcript profile model.....' + "\n"))
			errlog.info(print('Simulating empirical transcript profile' + "\n"))

			profile = process_inputFiles.proc_tx_expmodel(countModel)
			sample_trans_ids.append(profile[0])
			COUNTS.append(profile[1])

		elif countModel != None and readtot != None:
			# counts_s = np.rint(np.array([i * readtot for i in profile_propcount]) + 0.5).astype(int)
			profile = process_inputFiles.proc_tx_expmodel(countModel)
			counts_s = np.rint(np.multiply(profile[2], readtot)).astype(int)
			COUNTS.append(counts_s)
			sample_trans_ids.append(profile[0])

		for j in sample_trans_ids:
			p = processTransIDs(j)
			for id, seq in p.items():
				ID.append(id)
				Seq.append(seq)
		with gzip.open(output + '.fastq.gz', 'wb') as handle:
			for seq, r in zip(Seq, COUNTS[0]):

				readinfo = GenerateRead(seq, readlen, r, 'SE')
				startpos = readinfo[0]
				endpos = readinfo[1]

				for index, (i, j) in enumerate(zip(startpos[0], endpos[0])):
					header = sequence_identifier(index)
					read = seq[int(i):int(j)]
					q = sample_qualscore(sequencingModel=sqmodel)
					handle.write('{}\n{}\n+\n{}\n'.format(header, read, q).encode())

	elif args.pe:

		sample_trans_ids = []
		RFS = []
		COUNTS_P = []
		ID = []
		Seq = []
		R1 = []
		R2 = []

		if countModel == None:
			errlog.info(print('Generating paired-end reads.....' + "\n"))
			errlog.info(print('Sampling counts from negative binomial model' + "\n"))

			NB_counts = distributions.negative_binomial()
			counts_NB = np.random.choice(NB_counts, size=readtot, replace=True).tolist()
			counts_p = scalereadnum(counts_NB, readtot)
			COUNTS_P.append(counts_p)
			sample_trans_ids.append(random.choices(ref_transcript_ids, k=len(COUNTS_P[0])))

		elif countModel != None and readtot == None:
			errlog.info(print('Generating paired-end reads' + "\n"))
			errlog.info(print('Simulating empirical transcript profile.....' + "\n"))
			profile = process_inputFiles.proc_tx_expmodel(countModel)
			COUNTS_P.append(profile[1])
			sample_trans_ids.append(profile[0])

		elif countModel != None and readtot != None:
			errlog.info(print('Generating paired-end reads' + "\n"))
			errlog.info(print('Simulating empirical transcript profile.....' + "\n"))
			profile = process_inputFiles.proc_tx_expmodel(countModel)
			counts_p = np.rint(np.multiply(profile[2], readtot)).astype(int)
			COUNTS_P.append(counts_p)
			sample_trans_ids.append(profile[0])

		if fl_model != None:
			FS = process_inputFiles.proc_FLmodel(fl_model, readtot).astype(int)
			for i in COUNTS_P[0]:
					randomFS = random.choices(FS, k=i)
					RFS.append(randomFS)
		else:
			FS = np.random.normal(fragment_size, fragment_std, 100000).astype(int).tolist()
			for i in COUNTS_P[0]:
					randomFS = random.choices(FS, k=i)
					RFS.append(randomFS)

		data = list(itertools.chain.from_iterable(sample_trans_ids))
		for j in data:
			p = processTransIDs([j])
			for id, seq in p.items():
				ID.append(id)
				Seq.append(seq)

		for seq, r in zip(Seq, RFS):
			readinfo = GenerateRead(seq, r, len(r), 'PE')
			startpos = readinfo[0]
			endpos = readinfo[1]
			for index, (i, j) in enumerate(zip(startpos[0], endpos[0])):
				read = seq[int(i):int(j)]
				data = process_reads_PE(read, index)
				R1.append(''.join(data[0]))
				R2.append(''.join(data[1]))
		with gzip.open(output + '_R1.fastq.gz', 'wb') as f1, gzip.open(output + '_R2.fastq.gz', 'wb') as f2:
			for index, (i, j) in enumerate(zip(R1, R2)):
				id = sequence_identifier(index)
				q1 = sample_qualscore(sequencingModel=sqmodel)
				f1.write('{}\n{}\n+\n{}\n'.format(id, i, q1).encode())
				q2 = sample_qualscore(sequencingModel=sqmodel)
				f2.write('{}\n{}\n+\n{}\n'.format(id, j, q2).encode())
	os.remove(basename)
	os.remove(indexFile)

	errlog.info(print('Simulation is complete'))
	end_time = datetime.now()
	print('Duration: {}'.format(end_time - start_time))


if __name__ == '__main__':
	main()
